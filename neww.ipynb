{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n",
      "Device set to use 0\n",
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"fill-mask\", model=\"google-bert/bert-base-uncased\")\n",
    "# Load model directly\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "\n",
    "# Load the tokenizer and TensorFlow model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoTokenizer, TFAutoModelForMaskedLM\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load the tokenizer and the TensorFlow model\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the tokenizer and the TensorFlow model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model = TFAutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "\n",
    "# Input text with a [MASK] token\n",
    "input_text = \"The capital of France is [MASK].\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "\n",
    "# Get model predictions\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the logits for the masked token\n",
    "logits = outputs.logits\n",
    "\n",
    "# Find the index of the masked token\n",
    "masked_index = tf.where(inputs.input_ids == tokenizer.mask_token_id)[0][1].numpy()\n",
    "\n",
    "# Get the predicted token ID\n",
    "predicted_token_id = tf.argmax(logits[0, masked_index]).numpy()\n",
    "\n",
    "# Decode the predicted token ID to get the word\n",
    "predicted_word = tokenizer.decode([predicted_token_id])\n",
    "\n",
    "print(f\"The predicted word is: {predicted_word}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted word(s) for the masked token(s) is/are: ['[UNK]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to load the appropriate model based on selected language\n",
    "def load_model(language='en'):\n",
    "    if language == 'en':\n",
    "        model_name = \"bert-base-uncased\"  # English BERT model\n",
    "    elif language == 'es':\n",
    "        model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"  # Spanish BERT model\n",
    "    else:\n",
    "        print(\"Language not supported!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to predict the masked word in the sentence\n",
    "def predict_masked_word(input_text, language='en'):\n",
    "    # Load the model and tokenizer based on the selected language\n",
    "    tokenizer, model = load_model(language)\n",
    "    \n",
    "    if not tokenizer or not model:\n",
    "        return \"Error loading model for the selected language.\"\n",
    "    \n",
    "    # Replace '___' in the input with [MASK]\n",
    "    input_text = input_text.replace('___', '[MASK]')  # Replace placeholder ___ with [MASK]\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits for the masked token\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Find all indices of the [MASK] token(s)\n",
    "    masked_indices = tf.where(inputs.input_ids == tokenizer.mask_token_id)\n",
    "    \n",
    "    # If no [MASK] token found, return an error message\n",
    "    if len(masked_indices) == 0:\n",
    "        return \"No [MASK] token found in the input sentence.\"\n",
    "    \n",
    "    # Process each masked token (in case there are multiple [MASK] tokens)\n",
    "    predicted_words = []\n",
    "    for masked_index in masked_indices.numpy():\n",
    "        # Convert the index to a token ID and predict the word\n",
    "        predicted_token_id = tf.argmax(logits[0, masked_index[0]]).numpy()\n",
    "        predicted_word = tokenizer.decode([predicted_token_id])\n",
    "        predicted_words.append(predicted_word)\n",
    "\n",
    "    return predicted_words\n",
    "\n",
    "# Get user input for sentence and language selection\n",
    "input_text = input(\"Enter a sentence with '___' where the word is missing: \")\n",
    "language = input(\"Enter language code (en for English, es for Spanish): \")\n",
    "\n",
    "# Call the function to predict the masked word\n",
    "predicted_word = predict_masked_word(input_text, language)\n",
    "\n",
    "# Output the prediction\n",
    "print(f\"The predicted word(s) for the masked token(s) is/are: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input after replacing placeholder: The chef is known for his delicious [MASK].\n",
      "Tokenized input (input_ids): [[    4  2258 14363  1823 11160  1004 30959  1489  1837  9884 24035 30958\n",
      "      0  1008     5]]\n",
      "Logits shape: (1, 15, 31002)\n",
      "Masked indices: [[ 0 12]]\n",
      "Predicted token id: 3, Predicted word: [UNK]\n",
      "The predicted word(s) for the masked token(s) is/are: ['[UNK]']\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to load the appropriate model based on selected language\n",
    "def load_model(language='en'):\n",
    "    if language == 'en':\n",
    "        model_name = \"bert-base-uncased\"  # English BERT model\n",
    "    elif language == 'es':\n",
    "        model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"  # Spanish BERT model\n",
    "    else:\n",
    "        print(\"Language not supported!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to predict the masked word in the sentence\n",
    "def predict_masked_word(input_text, language='en', true_word=None):\n",
    "    # Load the model and tokenizer based on the selected language\n",
    "    tokenizer, model = load_model(language)\n",
    "    \n",
    "    if not tokenizer or not model:\n",
    "        return \"Error loading model for the selected language.\"\n",
    "    \n",
    "    # Replace '___' in the input with [MASK]\n",
    "    input_text = input_text.replace('___', '[MASK]')  # Replace placeholder ___ with [MASK]\n",
    "    print(f\"Input after replacing placeholder: {input_text}\")  # Debugging line\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    print(f\"Tokenized input (input_ids): {inputs['input_ids']}\")  # Debugging line\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits for the masked token\n",
    "    logits = outputs.logits\n",
    "    print(f\"Logits shape: {logits.shape}\")  # Debugging line\n",
    "    \n",
    "    # Find all indices of the [MASK] token(s)\n",
    "    masked_indices = tf.where(inputs.input_ids == tokenizer.mask_token_id)\n",
    "    print(f\"Masked indices: {masked_indices.numpy()}\")  # Debugging line\n",
    "    \n",
    "    # If no [MASK] token found, return an error message\n",
    "    if len(masked_indices) == 0:\n",
    "        return \"No [MASK] token found in the input sentence.\"\n",
    "    \n",
    "    # Process each masked token (in case there are multiple [MASK] tokens)\n",
    "    predicted_words = []\n",
    "    for masked_index in masked_indices.numpy():\n",
    "        # Convert the index to a token ID and predict the word\n",
    "        predicted_token_id = tf.argmax(logits[0, masked_index[0]]).numpy()\n",
    "        predicted_word = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"Predicted token id: {predicted_token_id}, Predicted word: {predicted_word}\")  # Debugging line\n",
    "        predicted_words.append(predicted_word)\n",
    "    \n",
    "    # Calculate accuracy if the true_word is provided\n",
    "    accuracy = None\n",
    "    if true_word:\n",
    "        # Calculate accuracy: 1 if match, 0 if no match\n",
    "        accuracy = sum([1 if predicted_word.strip().lower() == true_word.strip().lower() else 0 for predicted_word in predicted_words]) / len(predicted_words)\n",
    "    \n",
    "    return predicted_words, accuracy\n",
    "\n",
    "# Get user input for sentence, language selection, and true word (for accuracy calculation)\n",
    "input_text = input(\"Enter a sentence with '___' where the word is missing: \")\n",
    "language = input(\"Enter language code (en for English, es for Spanish): \")\n",
    "true_word = input(\"Enter the true word (if known): \")\n",
    "\n",
    "# Call the function to predict the masked word and calculate accuracy\n",
    "predicted_word, accuracy = predict_masked_word(input_text, language, true_word)\n",
    "\n",
    "# Output the prediction and accuracy\n",
    "if accuracy is not None:\n",
    "    print(f\"The predicted word(s) for the masked token(s) is/are: {predicted_word}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "else:\n",
    "    print(f\"The predicted word(s) for the masked token(s) is/are: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the layers of TFBertForMaskedLM were initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input after replacing placeholder: The chef is known for his delicious [MASK]\n",
      "Tokenized input (input_ids): [[    4  2258 14363  1823 11160  1004 30959  1489  1837  9884 24035 30958\n",
      "      0     5]]\n",
      "Logits shape: (1, 14, 31002)\n",
      "Masked indices: [[ 0 12]]\n",
      "Predicted token id: 3, Predicted word: [UNK]\n",
      "The predicted word(s) for the masked token(s) is/are: ['[UNK]']\n",
      "Accuracy: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# Function to load the appropriate model based on selected language\n",
    "def load_model(language='en'):\n",
    "    if language == 'en':\n",
    "        model_name = \"bert-base-uncased\"  # English BERT model\n",
    "    elif language == 'es':\n",
    "        model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"  # Spanish BERT model\n",
    "    else:\n",
    "        print(\"Language not supported!\")\n",
    "        return None, None\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to predict the masked word in the sentence\n",
    "def predict_masked_word(input_text, language='en', true_word=None):\n",
    "    # Load the model and tokenizer based on the selected language\n",
    "    tokenizer, model = load_model(language)\n",
    "    \n",
    "    if not tokenizer or not model:\n",
    "        return \"Error loading model for the selected language.\"\n",
    "    \n",
    "    # Replace '___' in the input with [MASK]\n",
    "    input_text = input_text.replace('___', '[MASK]')  # Replace placeholder ___ with [MASK]\n",
    "    print(f\"Input after replacing placeholder: {input_text}\")  # Debugging line\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    print(f\"Tokenized input (input_ids): {inputs['input_ids']}\")  # Debugging line\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits for the masked token\n",
    "    logits = outputs.logits\n",
    "    print(f\"Logits shape: {logits.shape}\")  # Debugging line\n",
    "    \n",
    "    # Find all indices of the [MASK] token(s)\n",
    "    masked_indices = tf.where(inputs.input_ids == tokenizer.mask_token_id)\n",
    "    print(f\"Masked indices: {masked_indices.numpy()}\")  # Debugging line\n",
    "    \n",
    "    # If no [MASK] token found, return an error message\n",
    "    if len(masked_indices) == 0:\n",
    "        return \"No [MASK] token found in the input sentence.\"\n",
    "    \n",
    "    # Process each masked token (in case there are multiple [MASK] tokens)\n",
    "    predicted_words = []\n",
    "    for masked_index in masked_indices.numpy():\n",
    "        # Convert the index to a token ID and predict the word\n",
    "        predicted_token_id = tf.argmax(logits[0, masked_index[0]]).numpy()\n",
    "        predicted_word = tokenizer.decode([predicted_token_id])\n",
    "        print(f\"Predicted token id: {predicted_token_id}, Predicted word: {predicted_word}\")  # Debugging line\n",
    "        predicted_words.append(predicted_word)\n",
    "    \n",
    "    # Calculate accuracy if the true_word is provided\n",
    "    accuracy = None\n",
    "    if true_word:\n",
    "        # Calculate accuracy: 1 if match, 0 if no match\n",
    "        accuracy = sum([1 if predicted_word.strip().lower() == true_word.strip().lower() else 0 for predicted_word in predicted_words]) / len(predicted_words)\n",
    "    \n",
    "    return predicted_words, accuracy\n",
    "\n",
    "# Get user input for sentence, language selection, and true word (for accuracy calculation)\n",
    "input_text = input(\"Enter a sentence with '___' where the word is missing: \")\n",
    "language = input(\"Enter language code (en for English, es for Spanish): \")\n",
    "true_word = input(\"Enter the true word (if known): \")\n",
    "\n",
    "# Call the function to predict the masked word and calculate accuracy\n",
    "predicted_word, accuracy = predict_masked_word(input_text, language, true_word)\n",
    "\n",
    "# Output the prediction and accuracy\n",
    "if accuracy is not None:\n",
    "    print(f\"The predicted word(s) for the masked token(s) is/are: {predicted_word}\")\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "else:\n",
    "    print(f\"The predicted word(s) for the masked token(s) is/are: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\DAAI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\DAAI\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [122344]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:55352 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:55352 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:55374 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:55408 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55433 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55828 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55828 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55828 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55828 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55828 - \"GET /predict HTTP/1.1\" 405 Method Not Allowed\n",
      "INFO:     127.0.0.1:55829 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:55829 - \"GET / HTTP/1.1\" 404 Not Found\n"
     ]
    }
   ],
   "source": [
    "import uvicorn\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "from transformers import AutoTokenizer, TFAutoModelForMaskedLM\n",
    "import tensorflow as tf\n",
    "\n",
    "# FastAPI instance\n",
    "app = FastAPI()\n",
    "\n",
    "# Function to load the appropriate model based on selected language\n",
    "def load_model(language='en'):\n",
    "    if language == 'en':\n",
    "        model_name = \"bert-base-uncased\"  # English BERT model\n",
    "    elif language == 'es':\n",
    "        model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"  # Spanish BERT model\n",
    "    else:\n",
    "        return None, None\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Function to predict the masked word in the sentence\n",
    "def predict_masked_word(input_text, language='en'):\n",
    "    tokenizer, model = load_model(language)\n",
    "    \n",
    "    if not tokenizer or not model:\n",
    "        raise HTTPException(status_code=400, detail=\"Model loading failed for the selected language.\")\n",
    "    \n",
    "    # Replace '___' in the input with [MASK]\n",
    "    input_text = input_text.replace('___', '[MASK]')  # Replace placeholder ___ with [MASK]\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(input_text, return_tensors=\"tf\")\n",
    "    \n",
    "    # Get model predictions\n",
    "    outputs = model(**inputs)\n",
    "    \n",
    "    # Get the logits for the masked token\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Find all indices of the [MASK] token(s)\n",
    "    masked_indices = tf.where(inputs.input_ids == tokenizer.mask_token_id)\n",
    "    \n",
    "    # If no [MASK] token found, return an error message\n",
    "    if len(masked_indices) == 0:\n",
    "        raise HTTPException(status_code=400, detail=\"No [MASK] token found in the input sentence.\")\n",
    "    \n",
    "    # Process each masked token (in case there are multiple [MASK] tokens)\n",
    "    predicted_words = []\n",
    "    for masked_index in masked_indices.numpy():\n",
    "        # Convert the index to a token ID and predict the word\n",
    "        predicted_token_id = tf.argmax(logits[0, masked_index[0]]).numpy()\n",
    "        predicted_word = tokenizer.decode([predicted_token_id])\n",
    "        predicted_words.append(predicted_word)\n",
    "\n",
    "    return predicted_words\n",
    "\n",
    "# Pydantic model to handle input structure\n",
    "class PredictionRequest(BaseModel):\n",
    "    input_text: str\n",
    "    language: str\n",
    "\n",
    "# API route to predict masked word\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: PredictionRequest):\n",
    "    try:\n",
    "        predicted_words = predict_masked_word(request.input_text, request.language)\n",
    "        return {\"predicted_words\": predicted_words}\n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "\n",
    "# To run the app in Jupyter or IPython environments:\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        # Check if the event loop is already running (useful in Jupyter Notebooks or environments with an active loop)\n",
    "        import nest_asyncio\n",
    "        nest_asyncio.apply()  # This allows us to run uvicorn in environments with a running event loop (like Jupyter)\n",
    "        \n",
    "        # Start the FastAPI app with Uvicorn\n",
    "        uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "    except RuntimeError as e:\n",
    "        if 'This event loop is already running' in str(e):\n",
    "            print(\"App is running in a Jupyter environment, skipping uvicorn.run() call.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
